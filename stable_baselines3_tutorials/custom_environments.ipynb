{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoxOjIlOImwx"
   },
   "source": "# Creating a custom Gym environment"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzevZcgmJmhi"
   },
   "source": [
    "## First steps with the gym interface\n",
    "\n",
    "As you have noticed in the previous notebooks, an environment that follows the gym interface is quite simple to use.\n",
    "It provides to this user mainly three methods, which have the following signature (for gym versions > 0.26)\n",
    "- `reset()` called at the beginning of an episode, it returns an observation and a dictionary with additional info (defaults to an empty dict)\n",
    "- `step(action)` called to take an action with the environment, it returns the next observation, the immediate reward, whether new state is a terminal state (episode is finished), whether the max number of timesteps is reached (episode is artificially finished), and additional information\n",
    "- (Optional) `render()` which allow to visualize the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we have to rely on `render_mode='rbg_array'` to retrieve an image of the scene).\n",
    "\n",
    "Under the hood, it also contains two useful properties:\n",
    "- `observation_space` which one of the gym spaces (`Discrete`, `Box`, ...) and describe the type and shape of the observation\n",
    "- `action_space` which is also a gym space object that describes the action space, so the type of action that can be taken\n",
    "\n",
    "The best way to learn about [gym spaces](https://gymnasium.farama.org/api/spaces/) is to look at the [source code](https://github.com/Farama-Foundation/Gymnasium/tree/main/gymnasium/spaces), but you need to know at least the main ones:\n",
    "- `gym.spaces.Box`: A (possibly unbounded) box in $R^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of [a, b], (-oo, b], [a, oo), or (-oo, oo). Example: A 1D-Vector or an image observation can be described with the Box space.\n",
    "```python\n",
    "# Example for using image as input:\n",
    "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
    "```                                       \n",
    "\n",
    "- `gym.spaces.Discrete`: A discrete space in $\\{ 0, 1, \\dots, n-1 \\}$\n",
    "  Example: if you have two actions (\"left\" and \"right\") you can represent your action space using `Discrete(2)`, the first action will be 0 and the second 1.\n",
    "\n",
    "\n",
    "[Documentation on custom env](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)\n",
    "\n",
    "Also keep in mind that Stabe-baselines internally uses the previous gym API (<0.26), so every VecEnv returns only the observation after resetting and returns a 4-tuple instead of a 5-tuple  (terminated & truncated are already combined to done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I98IKKyNJl6K"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Box(4,) means that it is a Vector with 4 components\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Shape:\", env.observation_space.shape)\n",
    "\n",
    "# Discrete(2) means that there is two discrete actions\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# The reset method is called at the beginning of an episode\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Sample a random action\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action:\", action)\n",
    "\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# Note the obs is a numpy array\n",
    "# info is an empty dict for now but can contain any debugging info\n",
    "# reward is a scalar\n",
    "print(obs.shape, reward, terminated, truncated, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqxatIwPOXe_"
   },
   "source": [
    "##  Gym env skeleton\n",
    "\n",
    "In practice this is how a gym environment looks like.\n",
    "Here, we have implemented a simple grid world were the agent must learn to go always left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYzDXA9vJfz1"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "class GoLeftEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is a simple env where the agent must learn to go always left.\n",
    "    \"\"\"\n",
    "\n",
    "    # If you don't want to use GUI, use \"console\" render mode\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    # Define constants for clearer code\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "    def __init__(self, grid_size=10, render_mode=\"console\"):\n",
    "        super(GoLeftEnv, self).__init__()\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Size of the 1D-grid\n",
    "        self.grid_size = grid_size\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos = grid_size - 1\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions, we have two: left and right\n",
    "        n_actions = 2\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "        # The observation will be the coordinate of the agent\n",
    "        # this can be described both by Discrete and Box space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=self.grid_size, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array)\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed, options=options)\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos = self.grid_size - 1\n",
    "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "        return np.array([self.agent_pos]).astype(np.float32), {}  # empty info dict\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Received invalid action={action} which is not part of the action space\"\n",
    "            )\n",
    "\n",
    "        # Account for the boundaries of the grid\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "        # Are we at the left of the grid?\n",
    "        terminated = bool(self.agent_pos == 0)\n",
    "        truncated = False  # we do not limit the number of steps here\n",
    "\n",
    "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "        reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "        # Optionally, we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            np.array([self.agent_pos]).astype(np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        # agent is represented as a cross, rest as a dot\n",
    "        if self.render_mode == \"console\":\n",
    "            print(\".\" * self.agent_pos, end=\"\")\n",
    "            print(\"x\", end=\"\")\n",
    "            print(\".\" * (self.grid_size - self.agent_pos))\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy5mlho1-Ine"
   },
   "source": [
    "### Validate the environment\n",
    "\n",
    "Stable Baselines3 provides a [helper](https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html) to check that your environment follows the Gym interface. It also optionally checks that the environment is compatible with Stable-Baselines (and emits warning if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DOpP_B0-LXm"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CcUVatq-P0l"
   },
   "outputs": [],
   "source": [
    "env = GoLeftEnv()\n",
    "# If the environment doesn't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ3khFtkSE0g"
   },
   "source": [
    "### Testing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i62yf2LvSAYY"
   },
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "env = GoLeftEnv(grid_size=10)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())\n",
    "\n",
    "GO_LEFT = 0\n",
    "# Hardcoded the best agent: always go left!\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "    print(f\"Step {step + 1}\")\n",
    "    obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
    "    done = terminated or truncated\n",
    "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv1e1qJETfHU"
   },
   "source": [
    "### Try it with Stable-Baselines\n",
    "\n",
    "Once your environment follow the gym interface, it is quite easy to plug in any algorithm from stable-baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQfLBE28SNDr"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "vec_env = make_vec_env(GoLeftEnv, n_envs=1, env_kwargs=dict(grid_size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRV4Q7FVUKB6"
   },
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1).learn(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJbeiF0RUN-p"
   },
   "outputs": [],
   "source": [
    "# Test the trained agent\n",
    "# using the vecenv\n",
    "obs = vec_env.reset()\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    print(f\"Step {step + 1}\")\n",
    "    print(\"Action: \", action)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        # Note that the VecEnv resets automatically\n",
    "        # when a done signal is encountered\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Register the environment\n",
    "\n",
    "Optionally, you can also register the environment with gym, that will allow you to create the RL agent in one line (and use `gym.make()` to instantiate the env):"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from gymnasium.envs.registration import register\n",
    "\n",
    "# Example for the CartPole environment\n",
    "register(\n",
    "    # unique identifier for the env `name-version`\n",
    "    id=\"CartPole-v1\",\n",
    "    # path to the class for creating the env\n",
    "    # Note: entry_point also accept a class as input (and not only a string)\n",
    "    entry_point=\"gym.envs.classic_control:CartPoleEnv\",\n",
    "    # Max number of steps per episode, using a `TimeLimitWrapper`\n",
    "    max_episode_steps=500,\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creating a more complex custom environment"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Games tend to make good environments, so I think a Snake game could be quite fitting. I searched around for a nice short/simple Snake game, and I found: https://github.com/TheAILearner/Snake-Game-using-OpenCV-Python/blob/master/snake_game_using_opencv.ipynb\n",
    "\n",
    "I took the notebook and converted it to a script here:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# source: https://github.com/TheAILearner/Snake-Game-using-OpenCV-Python/blob/master/snake_game_using_opencv.ipynb\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "\tapple_position = [random.randrange(1,50)*10,random.randrange(1,50)*10]\n",
    "\tscore += 1\n",
    "\treturn apple_position, score\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "\tif snake_head[0]>=500 or snake_head[0]<0 or snake_head[1]>=500 or snake_head[1]<0 :\n",
    "\t\treturn 1\n",
    "\telse:\n",
    "\t\treturn 0\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "\tsnake_head = snake_position[0]\n",
    "\tif snake_head in snake_position[1:]:\n",
    "\t\treturn 1\n",
    "\telse:\n",
    "\t\treturn 0\n",
    "\n",
    "img = np.zeros((500,500,3),dtype='uint8')\n",
    "# Initial Snake and Apple position\n",
    "snake_position = [[250,250],[240,250],[230,250]]\n",
    "apple_position = [random.randrange(1,50)*10,random.randrange(1,50)*10]\n",
    "score = 0\n",
    "prev_button_direction = 1\n",
    "button_direction = 1\n",
    "snake_head = [250,250]\n",
    "while True:\n",
    "\tcv2.imshow('a',img)\n",
    "\tcv2.waitKey(1)\n",
    "\timg = np.zeros((500,500,3),dtype='uint8')\n",
    "\t# Display Apple\n",
    "\tcv2.rectangle(img,(apple_position[0],apple_position[1]),(apple_position[0]+10,apple_position[1]+10),(0,0,255),3)\n",
    "\t# Display Snake\n",
    "\tfor position in snake_position:\n",
    "\t\tcv2.rectangle(img,(position[0],position[1]),(position[0]+10,position[1]+10),(0,255,0),3)\n",
    "\t\n",
    "\t# Takes step after fixed time\n",
    "\tt_end = time.time() + 0.05\n",
    "\tk = -1\n",
    "\twhile time.time() < t_end:\n",
    "\t\tif k == -1:\n",
    "\t\t\tk = cv2.waitKey(1)\n",
    "\t\telse:\n",
    "\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t# 0-Left, 1-Right, 3-Up, 2-Down, q-Break\n",
    "\t# a-Left, d-Right, w-Up, s-Down\n",
    "\n",
    "\tif k == ord('a') and prev_button_direction != 1:\n",
    "\t\tbutton_direction = 0\n",
    "\telif k == ord('d') and prev_button_direction != 0:\n",
    "\t\tbutton_direction = 1\n",
    "\telif k == ord('w') and prev_button_direction != 2:\n",
    "\t\tbutton_direction = 3\n",
    "\telif k == ord('s') and prev_button_direction != 3:\n",
    "\t\tbutton_direction = 2\n",
    "\telif k == ord('q'):\n",
    "\t\tbreak\n",
    "\telse:\n",
    "\t\tbutton_direction = button_direction\n",
    "\tprev_button_direction = button_direction\n",
    "\n",
    "\t# Change the head position based on the button direction\n",
    "\tif button_direction == 1:\n",
    "\t\tsnake_head[0] += 10\n",
    "\telif button_direction == 0:\n",
    "\t\tsnake_head[0] -= 10\n",
    "\telif button_direction == 2:\n",
    "\t\tsnake_head[1] += 10\n",
    "\telif button_direction == 3:\n",
    "\t\tsnake_head[1] -= 10\n",
    "\n",
    "\t# Increase Snake length on eating apple\n",
    "\tif snake_head == apple_position:\n",
    "\t\tapple_position, score = collision_with_apple(apple_position, score)\n",
    "\t\tsnake_position.insert(0,list(snake_head))\n",
    "\n",
    "\telse:\n",
    "\t\tsnake_position.insert(0,list(snake_head))\n",
    "\t\tsnake_position.pop()\n",
    "\t\t\n",
    "\t# On collision kill the snake and print the score\n",
    "\tif collision_with_boundaries(snake_head) == 1 or collision_with_self(snake_position) == 1:\n",
    "\t\tfont = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\t\timg = np.zeros((500,500,3),dtype='uint8')\n",
    "\t\tcv2.putText(img,'Your Score is {}'.format(score),(140,250), font, 1,(255,255,255),2,cv2.LINE_AA)\n",
    "\t\tcv2.imshow('a',img)\n",
    "\t\tcv2.waitKey(0)\n",
    "\t\tbreak\n",
    "\t\t\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The main changes are around the snippet:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "t_end = time.time() + 0.2\n",
    "k = -1\n",
    "while time.time() < t_end:\n",
    "\tif k == -1:\n",
    "\t\tk = cv2.waitKey(125)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Changing 0.2 to more like 0.05 and the waitKey to 1. We want to step as quickly as possible here.\n",
    "\n",
    "![Alt](https://pythonprogramming.net/static/images/reinforcement-learning/snake-base-game.gif)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Playing this, it's a simple snake game where you attempt to get the apple without running into yourself or going out of bounds.\n",
    "\n",
    "Next, we convert this to a gym environment:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "SNAKE_LEN_GOAL = 30\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "\tapple_position = [random.randrange(1,50)*10,random.randrange(1,50)*10]\n",
    "\tscore += 1\n",
    "\treturn apple_position, score\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "\tif snake_head[0]>=500 or snake_head[0]<0 or snake_head[1]>=500 or snake_head[1]<0 :\n",
    "\t\treturn 1\n",
    "\telse:\n",
    "\t\treturn 0\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "\tsnake_head = snake_position[0]\n",
    "\tif snake_head in snake_position[1:]:\n",
    "\t\treturn 1\n",
    "\telse:\n",
    "\t\treturn 0\n",
    "\n",
    "\n",
    "class SnekEnv(gym.Env):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(SnekEnv, self).__init__()\n",
    "        \n",
    "\t\t# Define action and observation space\n",
    "\t\t# They must be gym.spaces objects\n",
    "\t\t# We use discrete actions, as button_direction in the previous code \\in [0, 1, 2, 3]\n",
    "\t\tself.action_space = spaces.Discrete(4)\n",
    "        \n",
    "\t\t# Example for using image as input (channel-first; channel-last also works):\n",
    "\t\tself.observation_space = spaces.Box(low=-500, high=500,\n",
    "\t\t\t\t\t\t\t\t\t\t\tshape=(5+SNAKE_LEN_GOAL,), dtype=np.float32)\n",
    "\n",
    "\tdef step(self, action):\n",
    "        # This is basically the original snake game code, just turned into OOP\n",
    "        \n",
    "        # self.prev_actions.append(action) tracks historical actions.\n",
    "\t\tself.prev_actions.append(action)\n",
    "        \n",
    "\t\tcv2.imshow('a',self.img)\n",
    "\t\tcv2.waitKey(1)\n",
    "\t\tself.img = np.zeros((500,500,3),dtype='uint8')\n",
    "\t\t# Display Apple\n",
    "\t\tcv2.rectangle(self.img,(self.apple_position[0],self.apple_position[1]),(self.apple_position[0]+10,self.apple_position[1]+10),(0,0,255),3)\n",
    "\t\t# Display Snake\n",
    "\t\tfor position in self.snake_position:\n",
    "\t\t\tcv2.rectangle(self.img,(position[0],position[1]),(position[0]+10,position[1]+10),(0,255,0),3)\n",
    "\t\t\n",
    "\t\t# Takes a step after fixed time\n",
    "\t\tt_end = time.time() + 0.05\n",
    "\t\tk = -1\n",
    "\t\twhile time.time() < t_end:\n",
    "\t\t\tif k == -1:\n",
    "\t\t\t\tk = cv2.waitKey(1)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\tbutton_direction = action\n",
    "\t\t# Change the head position based on the button direction\n",
    "\t\tif button_direction == 1:\n",
    "\t\t\tself.snake_head[0] += 10\n",
    "\t\telif button_direction == 0:\n",
    "\t\t\tself.snake_head[0] -= 10\n",
    "\t\telif button_direction == 2:\n",
    "\t\t\tself.snake_head[1] += 10\n",
    "\t\telif button_direction == 3:\n",
    "\t\t\tself.snake_head[1] -= 10\n",
    "\n",
    "\t\t# Increase Snake length on eating apple\n",
    "\t\tif self.snake_head == self.apple_position:\n",
    "\t\t\tself.apple_position, self.score = collision_with_apple(self.apple_position, self.score)\n",
    "\t\t\tself.snake_position.insert(0,list(self.snake_head))\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tself.snake_position.insert(0,list(self.snake_head))\n",
    "\t\t\tself.snake_position.pop()\n",
    "\t\t\n",
    "\t\t# On collision kill the snake and print the score\n",
    "\t\tif collision_with_boundaries(self.snake_head) == 1 or collision_with_self(self.snake_position) == 1:\n",
    "\t\t\tfont = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\t\t\tself.img = np.zeros((500,500,3),dtype='uint8')\n",
    "\t\t\tcv2.putText(self.img,'Your Score is {}'.format(self.score),(140,250), font, 1,(255,255,255),2,cv2.LINE_AA)\n",
    "\t\t\tcv2.imshow('a',self.img)\n",
    "\t\t\tself.done = True\n",
    "\n",
    "\t\t# the reward is the snake's size\n",
    "\t\tself.total_reward = len(self.snake_position) - 3  # default length is 3\n",
    "\t\tself.reward = self.total_reward - self.prev_reward\n",
    "\t\tself.prev_reward = self.total_reward\n",
    "\n",
    "\t\tif self.done:\n",
    "\t\t\tself.reward = -10\n",
    "\t\tinfo = {}\n",
    "\n",
    "\t\thead_x = self.snake_head[0]\n",
    "\t\thead_y = self.snake_head[1]\n",
    "\n",
    "\t\tsnake_length = len(self.snake_position)\n",
    "\t\tapple_delta_x = self.apple_position[0] - head_x\n",
    "\t\tapple_delta_y = self.apple_position[1] - head_y\n",
    "\n",
    "\t\t# We now create an observation. We need to include the snake's head is, where the apple is, in relation to the head, and where the rest of the snake's body is.\n",
    "        # Feel free to make your custom actions. The only slightly challenging part is, every time you eat an apple, the length of the snake is increased by 1. We need our observation to be a fixed size, whether the snake is 3 units long, or 300.\n",
    "\t\tobservation = [head_x, head_y, apple_delta_x, apple_delta_y, snake_length] + list(self.prev_actions)\n",
    "\t\tobservation = np.array(observation)\n",
    "\n",
    "\t\treturn observation, self.reward, self.done, info\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.img = np.zeros((500,500,3),dtype='uint8')\n",
    "\t\t# Initial Snake and Apple position\n",
    "\t\tself.snake_position = [[250,250],[240,250],[230,250]]\n",
    "\t\tself.apple_position = [random.randrange(1,50)*10,random.randrange(1,50)*10]\n",
    "\t\tself.score = 0\n",
    "\t\tself.prev_button_direction = 1\n",
    "\t\tself.button_direction = 1\n",
    "\t\tself.snake_head = [250,250]\n",
    "\n",
    "\t\tself.prev_reward = 0\n",
    "\n",
    "\t\tself.done = False\n",
    "\n",
    "\t\thead_x = self.snake_head[0]\n",
    "\t\thead_y = self.snake_head[1]\n",
    "\n",
    "\t\tsnake_length = len(self.snake_position)\n",
    "\t\tapple_delta_x = self.apple_position[0] - head_x\n",
    "\t\tapple_delta_y = self.apple_position[1] - head_y\n",
    "\n",
    "\t\t# prev_actions: fixed-size list of previous actions that I expect the agent to be capable of figuring out how to extrapolate to where the rest of the body is based on \"snake length.\"\n",
    "\t\tself.prev_actions = deque(maxlen = SNAKE_LEN_GOAL)  # however long we aspire the snake to be\n",
    "\t\tfor i in range(SNAKE_LEN_GOAL):\n",
    "\t\t\tself.prev_actions.append(-1) # to create history\n",
    "\n",
    "\t\t# create observation:\n",
    "\t\tobservation = [head_x, head_y, apple_delta_x, apple_delta_y, snake_length] + list(self.prev_actions)\n",
    "\t\tobservation = np.array(observation)\n",
    "\n",
    "\t\treturn observation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now test our method:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "env = SnekEnv()\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We also make sure the rewards seem correct, the snake moves around, episodes end, and restart all as expected:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "env = SnekEnv()\n",
    "episodes = 50\n",
    "\n",
    "for episode in range(episodes):\n",
    "\tdone = False\n",
    "\tobs = env.reset()\n",
    "\twhile True:#not done:\n",
    "\t\trandom_action = env.action_space.sample()\n",
    "\t\tprint(\"action\",random_action)\n",
    "\t\tobs, reward, done, info = env.step(random_action)\n",
    "\t\tprint('reward',reward)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Time to try to train a model!"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "import time\n",
    "\n",
    "models_dir = f\"models/{int(time.time())}/\"\n",
    "logdir = f\"logs/{int(time.time())}/\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "\tos.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "\tos.makedirs(logdir)\n",
    "\n",
    "env = SnekEnv()\n",
    "env.reset()\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=logdir)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 0\n",
    "while True:\n",
    "\titers += 1\n",
    "\tmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO\")\n",
    "\tmodel.save(f\"{models_dir}/{TIMESTEPS*iters}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Go ahead and run it, and let's see what we can come up with!\n",
    "\n",
    "After training for some time, what we have is better than random, but is nowhere near being a great model. We can see that at least episode length increased, but our actual rewards are almost unchanged. In the next tutorial, we'll see if we can't figure out a solution!\n",
    "\n",
    "![alt](https://pythonprogramming.net/static/images/reinforcement-learning/custom-env-1.png)\n",
    "\n",
    "The resulting behaviour looks like:\n",
    "\n",
    "![alt](https://pythonprogramming.net/static/images/reinforcement-learning/base-custom-env.gif)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Engineering Rewards in Custom Environments"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "While the agent did definitely learn to stay alive for much longer than random, we were certainly not getting any apples. Why might this be?\n",
    "\n",
    "Unless an agent just happens to get an apple, it would never learn that it is rewarding, plus getting an apple is about as rewarding as simply not dying. How might we encourage getting the apple? One quick idea that comes to mind is to punish the agent by the euclidean distance it is from the apple. The agent would hopefully learn to get closer and closer to the apple in this case. We can achieve this by adding a euclidean distance variable and then subtracting that distance from the reward:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "euclidean_dist_to_apple = np.linalg.norm(np.array(self.snake_head) - np.array(self.apple_position))\n",
    "\n",
    "self.total_reward = len(self.snake_position) - 3 - euclidean_dist_to_apple"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Training this starts off okay, but after some time, all we get is a black screen and output like:\n",
    "\n",
    "```\n",
    "---------------------------------\n",
    "| rollout/           |          |\n",
    "|    ep_len_mean     | 1        |\n",
    "|    ep_rew_mean     | -10      |\n",
    "| time/              |          |\n",
    "|    fps             | 693      |\n",
    "|    iterations      | 1        |\n",
    "|    time_elapsed    | 121      |\n",
    "|    total_timesteps | 83968    |\n",
    "---------------------------------\n",
    "```\n",
    "\n",
    "What happened?\n",
    "\n",
    "![alt](https://pythonprogramming.net/static/images/reinforcement-learning/custom-env-oops.png)\n",
    "\n",
    "Quite simply, this agent learned that living is very painful and the quickest way to the highest reward is to go ahead and stop living. You can see that reward was typically a very large negative and then it rises as episode length decreases up to the point of -10 and it just holds there, so the agent was just simply spawning and running into itself immediately to end the game. This is a good example of how things can go awry with what we think might be a good reward, but it turns out to be no good.\n",
    "\n",
    "To fix this, we can instead just make an offset for the euclidean distance. I propose something like maybe 250, since our game size is 500x500. When we do this, I can envision the snake learning to just circle the apple, instead of eating it. The new reward function I propose to start is:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "self.total_reward = (250 - euclidean_dist_to_apple)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "But then, we do want a short term reward for eating an apple too. It needs to be greater than 250 for sure, but also enough incentive for the apple to move to a new spot, so maybe 1,000 or 5,000. I really don't know. Something significant for sure. We have to consider how many steps will it take to get to the new/next apple, and would it wind up being more advantageous for the agent to just do circles around the apple for a constant ~200-250 reward. In the code where we catch if we ate the apple and lengthen the snake, I'll add a new variable, apple_reward:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "apple_reward = 0\n",
    "# Increase Snake length on eating apple\n",
    "if self.snake_head == self.apple_position:\n",
    "\tself.apple_position, self.score = collision_with_apple(self.apple_position, self.score)\n",
    "\tself.snake_position.insert(0,list(self.snake_head))\n",
    "\tapple_reward = 10000"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then, we'll add this apple_reward to the temp reward:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "self.total_reward = (250 - euclidean_dist_to_apple) + apple_reward"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Then, do we really want the rewards to be deltas to what they were before? I don't think so. I think we want to make them per step, so the reward we return from the step method should be the `self.total_reward`.\n",
    "\n",
    "Finally, our current total reward scale is going to be too massive; we should really scale it down; I propose for now that we divide it by 100."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "self.total_reward = ((250 - euclidean_dist_to_apple) + apple_reward)/100"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We now obtain the desired behaviour:\n",
    "\n",
    "![alt](https://pythonprogramming.net/static/images/reinforcement-learning/snake-modified-trained.gif)\n",
    "\n",
    "Full code is:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# our environment here is adapted from: https://github.com/TheAILearner/Snake-Game-using-OpenCV-Python/blob/master/snake_game_using_opencv.ipynb\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "SNAKE_LEN_GOAL = 30\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "    apple_position = [random.randrange(1,50)*10,random.randrange(1,50)*10]\n",
    "    score += 1\n",
    "    return apple_position, score\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if snake_head[0]>=500 or snake_head[0]<0 or snake_head[1]>=500 or snake_head[1]<0 :\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class SnekEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SnekEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=-500, high=500,\n",
    "                                            shape=(5+SNAKE_LEN_GOAL,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.prev_actions.append(action)\n",
    "        cv2.imshow('a',self.img)\n",
    "        cv2.waitKey(1)\n",
    "        self.img = np.zeros((500,500,3),dtype='uint8')\n",
    "        \n",
    "        # Display Apple\n",
    "        cv2.rectangle(self.img,(self.apple_position[0],self.apple_position[1]),(self.apple_position[0]+10,self.apple_position[1]+10),(0,0,255),3)\n",
    "        \n",
    "        # Display Snake\n",
    "        for position in self.snake_position:\n",
    "            cv2.rectangle(self.img,(position[0],position[1]),(position[0]+10,position[1]+10),(0,255,0),3)\n",
    "        \n",
    "        # Takes a step after fixed time\n",
    "        t_end = time.time() + 0.05\n",
    "        k = -1\n",
    "        while time.time() < t_end:\n",
    "            if k == -1:\n",
    "                k = cv2.waitKey(1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        button_direction = action\n",
    "        # Change the head position based on the button direction\n",
    "        if button_direction == 1:\n",
    "            self.snake_head[0] += 10\n",
    "        elif button_direction == 0:\n",
    "            self.snake_head[0] -= 10\n",
    "        elif button_direction == 2:\n",
    "            self.snake_head[1] += 10\n",
    "        elif button_direction == 3:\n",
    "            self.snake_head[1] -= 10\n",
    "\t\t\n",
    "        apple_reward = 0\n",
    "        # Increase Snake length on eating apple\n",
    "        if self.snake_head == self.apple_position:\n",
    "            self.apple_position, self.score = collision_with_apple(self.apple_position, self.score)\n",
    "            self.snake_position.insert(0,list(self.snake_head))\n",
    "            apple_reward = 10000\n",
    "        else:\n",
    "            self.snake_position.insert(0,list(self.snake_head))\n",
    "            self.snake_position.pop()\n",
    "        \n",
    "        # On collision kill the snake and print the score\n",
    "        if collision_with_boundaries(self.snake_head) == 1 or collision_with_self(self.snake_position) == 1:\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            self.img = np.zeros((500,500,3),dtype='uint8')\n",
    "            cv2.putText(self.img,'Your Score is {}'.format(self.score),(140,250), font, 1,(255,255,255),2,cv2.LINE_AA)\n",
    "            cv2.imshow('a',self.img)\n",
    "            self.done = True\n",
    "        \n",
    "        euclidean_dist_to_apple = np.linalg.norm(np.array(self.snake_head) - np.array(self.apple_position))\n",
    "\n",
    "        self.total_reward = ((250 - euclidean_dist_to_apple) + apple_reward)/100\n",
    "\n",
    "        print(self.total_reward)\n",
    "\n",
    "        self.reward = self.total_reward - self.prev_reward\n",
    "        self.prev_reward = self.total_reward\n",
    "\n",
    "        if self.done:\n",
    "            self.reward = -10\n",
    "        info = {}\n",
    "\n",
    "        head_x = self.snake_head[0]\n",
    "        head_y = self.snake_head[1]\n",
    "\n",
    "        snake_length = len(self.snake_position)\n",
    "        apple_delta_x = self.apple_position[0] - head_x\n",
    "        apple_delta_y = self.apple_position[1] - head_y\n",
    "\n",
    "        # create observation:\n",
    "        observation = [head_x, head_y, apple_delta_x, apple_delta_y, snake_length] + list(self.prev_actions)\n",
    "        observation = np.array(observation)\n",
    "\n",
    "        return observation, self.total_reward, self.done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.img = np.zeros((500,500,3),dtype='uint8')\n",
    "        # Initial Snake and Apple position\n",
    "        self.snake_position = [[250,250],[240,250],[230,250]]\n",
    "        self.apple_position = [random.randrange(1,50)*10,random.randrange(1,50)*10]\n",
    "        self.score = 0\n",
    "        self.prev_button_direction = 1\n",
    "        self.button_direction = 1\n",
    "        self.snake_head = [250,250]\n",
    "\n",
    "        self.prev_reward = 0\n",
    "\n",
    "        self.done = False\n",
    "\n",
    "        head_x = self.snake_head[0]\n",
    "        head_y = self.snake_head[1]\n",
    "\n",
    "        snake_length = len(self.snake_position)\n",
    "        apple_delta_x = self.apple_position[0] - head_x\n",
    "        apple_delta_y = self.apple_position[1] - head_y\n",
    "\n",
    "        self.prev_actions = deque(maxlen = SNAKE_LEN_GOAL)  # however long we aspire the snake to be\n",
    "        for i in range(SNAKE_LEN_GOAL):\n",
    "            self.prev_actions.append(-1) # to create history\n",
    "\n",
    "        # create observation:\n",
    "        observation = [head_x, head_y, apple_delta_x, apple_delta_y, snake_length] + list(self.prev_actions)\n",
    "        observation = np.array(observation)\n",
    "\n",
    "        return observation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "5.custom_gym_env.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3201c96db5836b171d01fee72ea1be894646622d4b41771abf25c98b548a611d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
